{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from src.utils import (\n",
    "    load_preprocess_data,\n",
    "    run_prediction,\n",
    "    extract_disease_names,\n",
    "    select_case_components_based_on_id)\n",
    "load_dotenv()\n",
    "from prompts import single_shot_disease_only_prompt,with_reasons_prompt,compare_others_prompt,self_refinement_prompt,compare_others_prompt_without_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath=\"dataset/clinicallab/data_en.json\"\n",
    "df=load_preprocess_data(filePath)\n",
    "df['differential_diagnosis'] = df['differential_diagnosis'].apply(extract_disease_names)\n",
    "filtered_df = df[df['differential_diagnosis'].apply(lambda x: len(x) > 1)]\n",
    "filtered_df['differential_diagnosis'] = filtered_df['differential_diagnosis'] + filtered_df['principal_diagnosis'].apply(lambda x: [x] if x else [])\n",
    "df=filtered_df\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 729/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import (\n",
    "#     run_prediction,\n",
    "# )\n",
    "# from prompts import single_shot_disease_only_prompt,with_reasons_prompt,compare_others_prompt,self_refinement_prompt\n",
    "# type=\"with_reasons\"\n",
    "# prompt=with_reasons_prompt\n",
    "# type=\"single_shot\"\n",
    "# prompt=single_shot_disease_only_prompt\n",
    "# # type=\"open_ended\"\n",
    "# # # prompt=open_ended__top4_prompt# \n",
    "# run_prediction(df,prompt,departments,models=models,type=type,skip=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from src.utils import evaluate_department_results\n",
    "types=[\"single_shot\",\"with_reasons\",\"check_others_input\"]\n",
    "\n",
    "for type in types:\n",
    "    folder=os.path.join(\"results\",type)\n",
    "    print(\"type is\",type)\n",
    "    all_files=os.listdir(folder)\n",
    "    all_results={}\n",
    "    for file in all_files:\n",
    "        department_name=file.split(\"_\")[0]\n",
    "        print(department_name)\n",
    "        if type in file:\n",
    "            file_path=os.path.join(folder,file)\n",
    "            department_name=file.split(\"_\")[0]\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                results=evaluate_department_results(data)\n",
    "            all_results[department_name]=results\n",
    "    \n",
    "    with open(f\"results/analysis/{type}.json\", \"w\") as outfile: \n",
    "        json.dump(all_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields=[ \"Patient basic information\",\n",
    "                 \"Chief complaint\",\n",
    "                 \"Medical history\",\n",
    "                 \"Physical examination\",\n",
    "                 \"Laboratory examination\",\n",
    "                 \"Imageological examination\",\n",
    "                 \"Auxillary examination\",\n",
    "                 \"Pathological examination\"\n",
    "\n",
    "]\n",
    "departments=[\"nephrology department\",\n",
    "            \"gynecology department\",\n",
    "            \"endocrinology department\",\n",
    "            \"neurology department\",\n",
    "             \"pediatrics department\",\n",
    "             \"cardiac surgical department\",\n",
    "             \"gastrointestinal surgical department\",\n",
    "             \"respiratory medicine department\",\n",
    "             \"gastroenterology department\",\n",
    "             \"urinary surgical department\",\n",
    "             \"hepatobiliary and pancreas surgical department\",\n",
    "             \"hematology department\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diagnosis_by_other_models_by_id(id,department_results,models):\n",
    "    all_diagnosis=[]\n",
    "    for model in models:\n",
    "        diagnosis=department_results[id][\"predictions\"][model]\n",
    "        diagnosis= diagnosis.replace(\"\\n\", \"\")\n",
    "        diagnosis= diagnosis.replace(\"```json\", \"\")\n",
    "        diagnosis= diagnosis.replace(\"```\", \"\")\n",
    "        all_diagnosis.append(diagnosis)\n",
    "    return all_diagnosis\n",
    "\n",
    "def compare_others_ollama(prompt,medical_history, modelname, diseases, department,doctor_diagnosis=[]):\n",
    "    model = OllamaLLM(model=modelname,temperature=0.1,num_predict=1500,num_ctx=4096)#1500\n",
    "\n",
    "    system_template = prompt\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "    chain = chat_prompt | model\n",
    "    if len(doctor_diagnosis)==1:\n",
    "        results=chain.invoke({\"department\": department,\n",
    "                            \"diseases\": diseases,\n",
    "                            \"medical_history\":json.dumps(medical_history),\n",
    "                            \"diagnosis\":doctor_diagnosis[0]})\n",
    "        return results\n",
    "    elif len(doctor_diagnosis)==2:\n",
    "        results=chain.invoke({\"department\": department,\n",
    "                            \"diseases\": diseases,\n",
    "                            \"medical_history\":json.dumps(medical_history),\n",
    "                            \"doctor_1_diagnosis\":doctor_diagnosis[0],\n",
    "                            \"doctor_2_diagnosis\":doctor_diagnosis[1]})\n",
    "        return results\n",
    "    elif len(doctor_diagnosis)==3:\n",
    "        results=chain.invoke({\"department\": department,\n",
    "                            \"diseases\": diseases,\n",
    "                            \"medical_history\":json.dumps(medical_history),\n",
    "                            \"doctor_1_diagnosis\":doctor_diagnosis[0],\n",
    "                            \"doctor_2_diagnosis\":doctor_diagnosis[1],\n",
    "                            \"doctor_3_diagnosis\":doctor_diagnosis[2]})\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def compare_others_gpt(prompt,medical_history, model, diseases, department,doctor_diagnosis=[]):\n",
    "    chat = ChatOpenAI(model_name=model, temperature=0.1,max_tokens=1500)\n",
    "    system_template = prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "    if len(doctor_diagnosis)==1:\n",
    "        messages = chat_prompt.format_prompt(\n",
    "            department=department,\n",
    "            diseases=diseases,\n",
    "            medical_history=json.dumps(medical_history),\n",
    "            diagnosis=doctor_diagnosis[0],\n",
    "\n",
    "        ).to_messages()\n",
    "    elif len(doctor_diagnosis)==2:\n",
    "        messages = chat_prompt.format_prompt(\n",
    "            department=department,\n",
    "            diseases=diseases,\n",
    "            medical_history=json.dumps(medical_history),\n",
    "            doctor_1_diagnosis=doctor_diagnosis[0],\n",
    "            doctor_2_diagnosis=doctor_diagnosis[1]\n",
    "\n",
    "        ).to_messages()\n",
    "    elif len(doctor_diagnosis)==3:\n",
    "        messages = chat_prompt.format_prompt(\n",
    "            department=department,\n",
    "            diseases=diseases,\n",
    "            medical_history=json.dumps(medical_history),\n",
    "            doctor_1_diagnosis=doctor_diagnosis[0],\n",
    "            doctor_2_diagnosis=doctor_diagnosis[1],\n",
    "            doctor_3_diagnosis=doctor_diagnosis[2],\n",
    "\n",
    "        ).to_messages()\n",
    "    response = chat(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check others including mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models= [\"gpt-4o\",\"llama3.1\",\"gemma2\"]\n",
    "all_models_to_check=[\"gpt-4o\",\"llama3.1\",\"gemma2\"]\n",
    "type=\"check_others_input\"\n",
    "prompt_type=compare_others_prompt\n",
    "for department in departments:\n",
    "    print(department)\n",
    "    results={}\n",
    "    with open(f\"results4/with_reasons/{department}_with_reasons.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    department_results=data\n",
    "    ids=list(department_results.keys())\n",
    "    print(ids)\n",
    "    for case_id in ids:\n",
    "        print(case_id)\n",
    "        case_details={}\n",
    "        principal_diagnosis,differential_diagnosis,filtered_clinical_case_dict=select_case_components_based_on_id(df,int(case_id),required_fields)\n",
    "        doctor_diagnosis=get_diagnosis_by_other_models_by_id(case_id,department_results,all_models)\n",
    "        case_details[\"original\"]={\"main-diagnosis\":principal_diagnosis,\"differential_diagnosis\":differential_diagnosis}\n",
    "        case_details[\"predictions\"]={}\n",
    "        for model in all_models_to_check:\n",
    "            if \"gpt\" in model:\n",
    "                output=compare_others_gpt(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis)\n",
    "            else:\n",
    "                output=compare_others_ollama(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis)\n",
    "            case_details[\"predictions\"][model]=output\n",
    "        results[str(case_id)]=case_details\n",
    "\n",
    "    with open(f\"results/{type}/{department}_{type}.json\", \"w\") as outfile:\n",
    "        json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models= [\"gpt-4o\"]\n",
    "all_models_to_check=[\"gpt-4o\"]\n",
    "type=\"self_refinement\"\n",
    "prompt_type=self_refinement_prompt\n",
    "for department in departments:\n",
    "    print(department)\n",
    "    results={}\n",
    "    with open(f\"results-all-models/with_reasons/{department}_with_reasons.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    department_results=data\n",
    "    ids=list(department_results.keys())\n",
    "    print(ids)\n",
    "    for case_id in ids:\n",
    "        print(case_id)\n",
    "        case_details={}\n",
    "        principal_diagnosis,differential_diagnosis,filtered_clinical_case_dict=select_case_components_based_on_id(df,int(case_id),required_fields)\n",
    "        doctor_diagnosis=get_diagnosis_by_other_models_by_id(case_id,department_results,all_models)\n",
    "        case_details[\"original\"]={\"main-diagnosis\":principal_diagnosis,\"differential_diagnosis\":differential_diagnosis}\n",
    "        case_details[\"predictions\"]={}\n",
    "        for i,model in enumerate(all_models_to_check):\n",
    "            doctor_diagnosis=[doctor_diagnosis[i]]\n",
    "            if \"gpt\" in model:\n",
    "                output=compare_others_gpt(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis)\n",
    "            else:\n",
    "                output=compare_others_ollama(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis)\n",
    "            case_details[\"predictions\"][model]=output\n",
    "        results[str(case_id)]=case_details\n",
    "    path = f\"results-self-refinement/{type}/{model}/{department}_{type}.json\"\n",
    "    folder_path = os.path.dirname(path)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    with open(path, \"w\") as outfile:\n",
    "      json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check others without mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models= [\"gpt-4o\",\"llama3.1\",\"gemma2\"]\n",
    "all_models_to_check=[\"gpt-4o\",\"llama3.1\",\"gemma2\"]\n",
    "type=\"check_others_input_without_mine\"\n",
    "prompt_type=compare_others_prompt_without_mine\n",
    "for department in departments:\n",
    "    print(department)\n",
    "    results={}\n",
    "    with open(f\"results4/with_reasons/{department}_with_reasons.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    department_results=data\n",
    "    ids=list(department_results.keys())\n",
    "    print(ids)\n",
    "    for case_id in ids:\n",
    "        print(case_id)\n",
    "        case_details={}\n",
    "        principal_diagnosis,differential_diagnosis,filtered_clinical_case_dict=select_case_components_based_on_id(df,int(case_id),required_fields)\n",
    "        doctor_diagnosis=get_diagnosis_by_other_models_by_id(case_id,department_results,all_models)\n",
    "        case_details[\"original\"]={\"main-diagnosis\":principal_diagnosis,\"differential_diagnosis\":differential_diagnosis}\n",
    "        case_details[\"predictions\"]={}\n",
    "        for i,model in enumerate(all_models_to_check):\n",
    "            doctor_diagnosis1=doctor_diagnosis[:]\n",
    "            if model==\"gpt-4o\":\n",
    "              continue\n",
    "            del doctor_diagnosis1[i]\n",
    "\n",
    "            if \"gpt\" in model:\n",
    "                output=compare_others_gpt(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis1)\n",
    "            else:\n",
    "                output=compare_others_ollama(prompt_type,filtered_clinical_case_dict,model,differential_diagnosis,department,doctor_diagnosis1)\n",
    "            case_details[\"predictions\"][model]=output\n",
    "        results[str(case_id)]=case_details\n",
    "\n",
    "    path = f\"results5/{type}/{model}/{department}_{type}.json\"\n",
    "    folder_path = os.path.dirname(path)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    with open(path, \"w\") as outfile:\n",
    "      json.dump(results, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
